<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-VP0MSHHZXV"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VP0MSHHZXV');
  </script>
  
  <script id="Cookiebot" src="https://consent.cookiebot.com/uc.js" data-cbid="1b90351d-faf2-476e-b4b0-41b46ba0dbd1" type="text/javascript" async></script>
  
  <meta name="google-site-verification" content="dHsBSr-E6ri97EAzrd5HHzPDB6WD7pr4M8WcBR1Glr8" />
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Visual Informative Part Attention Framework for Transformer-based Referring Image Segmentation">
  <meta property="og:title" content="VIPA: Visual Informative Part Attention Framework for Transformer-based Referring Image Segmentation"/>
  <meta property="og:description" content="Project page for VIPA"/>
  <meta name="og:image" content="static/images/vipa_intro.png">
  <meta property="og:url" content="https://yubin1219.github.io/VIPA/"/>
  


  <meta name="twitter:title" content="VIPA: Visual Informative Part Attention Framework for Transformer-based Referring Image Segmentation">
  <meta name="twitter:description" content="Visual Informative Part Attention Framework for Transformer-based Referring Image Segmentation">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/vipa_intro.png">
  <meta name="twitter:card" content="VIPA">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Transformer-based Referring Image Segmentation, Visual Informative Part-aware Attention, Visual Expression">
  
  
  <title>VIPA</title>
  <link rel="icon" type="image/x-icon" href="https://img.icons8.com/?size=100&id=UcdyUtGQsmqX&format=png&color=000000">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Visual Informative Part Attention Framework for Transformer-based Referring Image Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=qRjx9NkAAAAJ&hl=ko&oi=sra" target="_blank">Yubin Cho</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=c-4JTUkAAAAJ&hl=ko&oi=sra" target="_blank">Hyunwoo Yu</a><sup>2*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=O9QSF7UAAAAJ&hl=ko&oi=sra" target="_blank">Kyeongbo Kong</a><sup>3*</sup>,</span>
                    <span class="author-block">
                      <a href="https://www.lti.cs.cmu.edu/people/faculty/singh-rita.html" target="_blank">Rita Singh</a><sup>4&dagger;</sup>,</span>
                        <span class="author-block">
                        <a href="https://scholar.google.com/citations?user=3WYxpuYAAAAJ&hl=ko&oi=sra" target="_blank">Suk-Ju Kang</a><sup>2&dagger;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup> AI Lab, LG Electronics <sup>2</sup> Sogang University <sup>3</sup> Pusan National University <sup>4</sup> Carnegie Mellon University </span><br><sup>*</sup>Equal Contribution <sup>&dagger;</sup>Corresponding Author</span>
                  </div>
            
                  <!-- <div class="is-size-5 publication-authors" > -->
                    <!-- <p class="author-block" > <strong></strong> -->
                    <!-- </p> -->
                  <!-- </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://yubin1219.github.io/VIPA/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/yubin1219/vipa_ris" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://yubin1219.github.io/VIPA/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
            <div class="image"><br>
              <center>
            <img src="static/images/vipa_intro.png" alt="qual"/>
            </center> </div>
            <div class="content has-text-left">
              <strong>Figure 1. (a-c) Illustration of different RIS frameworks.</strong> Different from previous approaches, our approach exploits visual expression, generated from the retrieved informative parts of visual contexts, as a key-value set in the Transformer decoder for Transformer-based RIS. <strong>(d) Visual comparison of two different key-value sets.</strong> Yellow dotted boxes are incorrect predictions. The visual expression (VE) robustly guides the networkâ€™s attention to the regions of interest, whereas the advanced language expression (LE) shows incomplete prediction. 
            </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Referring Image Segmentation (RIS) aims to segment a target object described by a natural language expression. Existing methods typically adopt a Transformer-based decoder, where language-based tokens serve as a key-value set (i.e., an information provider) to assign attention to target regions for the vision query features. However, these frameworks guided by language-based tokens have an inherent limitation in that language information is insufficient to guide the network's attention towards fine-grained target regions, even with advanced language models. To address this issue, we propose a novel Visual Informative Part Attention (VIPA) framework that leverages visual expression, generated from the informative parts of visual contexts, as a key-value set in the decoder for Transformer-based referring image segmentation. The visual expression effectively provides the structural and semantic visual information to the network. We also design a visual expression generator (VEG) module, which retrieves informative visual tokens via local-global linguistic context cues and refines the retrieved tokens for reducing noise information and sharing informative visual attributes. This module allows visual expression to consider comprehensive contexts and capture semantic visual contexts of informative regions. In this way, our framework enables the network's attention to robustly align with the fine-grained regions of interest. Extensive experiments and visual analysis demonstrate the effectiveness of our approach. Our VIPA outperforms the existing state-of-the-art methods on four public RIS benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper highlight -->
<section class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ðŸ”¥ Highlights</h2>
        <div class="content has-text-left">
          <p>
            1. We propose a novel <strong><font color="#D2AB5B">Visual Informative Part Attention (VIPA) framework for Transformer-based referring image segmentation</font></strong>, which leverages the informative parts of visual contexts (i.e., visual expression) as a key-value set in the transformer decoder. The visual expression robustly leads the networkâ€™s attention to the region of interest. Our approach is the first to explore the potential of visual expression in the attention mechanism of referring image segmentation.
                        
            <br><br>
            2. We present a <strong><font color="#D2AB5B">visual expression generator (VEG) module</font></strong>, which retrieves informative visual tokens via local-global linguistic cues and refines them for mitigating the distraction by noise information and sharing the visual attributes. This module enables visual expression to consider comprehensive contexts and capture semantic visual contexts for fine-grained segmentation. 
          
          <br><br>
          3. Our method <strong><font color="#D2AB5B">consistently shows strong performance and surpasses the state-of-the-art methods</font></strong> on four public RIS benchmarks. The visual analysis of attention results clearly demonstrates the effectiveness of our framework for Transformer-based referring image segmentation.
          
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper highlight -->




<!-- Image carousel -->
<section class="section hero is-light">
  <div class="hero-body">
    <div class="container">
     <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
         <h2 class="title is-3">Overview Framework</h2>
          <div class="image">
            <!-- Your image here -->
            <center>
           <img src="static/images/vipa_architecture_figure.png" alt="Overall architecture"/>
            </center>
            </div>
            <div class="content has-text-left">
              <strong>Figure 2. Overview of Visual Informative Part Attention (VIPA) framework.</strong> Our approach robustly guides the network's attention to the region of interest by exploiting the visual expression generated from the retrieved informative parts of visual tokens. The Visual Expression Generator produces the visual expression via two steps: the visual informative token retrieval and the visual context refinement.
            </div>
         </div>
      </div>
  </div>
</div>
</section>
<!-- End image carousel -->

<!-- Experimental -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experimental Results</h2>
          <div class="image">
          <center>
            <img src="static/images/main_table_vipa.png" alt="table1"/>
          </center>
          </div>
          <div class="content has-text-left">
            <p><strong>Table 1.</strong> Performance comparison with the state-of-the-art methods using oIoU (%) metric on three public referring image segmentation datasets. LLM-based models are marked in gray. â€  indicates models trained on multiple RefCOCO series datasets with removed validation and testing images to prevent data leakage.  For ReferIt dataset, only ReferIt training set is used.</p>
          </div>
          <div class="image">
          <center>
            <img src="static/images/miou_table.png" alt="table2" style="width:800px;height:205px;" />
          </center>
          </div>
          <div class="content has-text-left">
            <p><strong>Table 2.</strong> Performance comparison of recent RIS methods using mIoU (%) metric. â€  indicates models trained on multiple RefCOCO series datasets.</p>
          </div>
          <div class="image">
          <center>
            <img src="static/images/main_ablation_vipa.png" alt="table3" />
          </center>
          </div>
          <div class="content has-text-centered">
            <p><strong>Table 3.</strong> Main ablation for the effectiveness of our method. LE: Linguistic Expression tokens. VE: Visual Expression tokens (Ours). </p>
          </div>
          <div class="image">
          <center>
            <img src="static/images/design_choice_table.png" alt="table4" style="width:620px;height:250px;" />
          </center>
          </div>
          <div class="content has-text-centered">
            <p><strong>Table 4.</strong> Ablation studies for the design of the visual expression generator module. </p>
          </div>
          <div class="image">
          <center>
            <img src="static/images/ablation_token_ratio.png" alt="table6"  />
          </center>
          </div>
          <div class="content has-text-centered">
            <p><strong>Figure 3.</strong> Ablation study on the number of the retrieved informative visual tokens. </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Experimental -->


<!-- Qualitative -->
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Visualizations</h2>
          <div id="results-carousel" class="carousel results-carousel">
           <div class="item">
            <!-- Your image here -->
            <img src="static/images/ablation_visualization.png" alt="ablation_visual"/>
            <h3 class="subtitle has-text-left">
              <small><strong>Figure 4. Visualized comparison with the ablation method (i.e., advanced Language Expression (LE)) and our method (i.e., Visual Expression (VE)).</strong></small> <small>Our method consistently predicted accurate regions for both large target regions and small target regions., whereas the ablation method inconsistently predicted the target regions and showed incorrect segmentation regions. </small>
            </h3>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/attention_map_compare.png" alt="attention"/>
            <h3 class="subtitle has-text-left">
              <small><strong>Figure 5. Visual analysis of the attention map between vision features and visual expression (VE), and the attention map between vision features and advanced language expression (LE).</strong></small> <small>Our visual expression robustly guides the networkâ€™s attention to the regions of interest, whereas the advanced language expression fails to focus on the real target regions or highlights even wide non-target regions.</small>
            </h3>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/visual_compare.png" alt="quali"/>
            <h3 class="subtitle has-text-left">
              <small><strong>Figure 6. Qualitative comparison with previous state-of-the-art methods for the different types of the images and language expressions.</strong></small> 
            </h3>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/visual_compare_with_LLM.png" alt="LLM_quali"/>
            <h3 class="subtitle has-text-centered">
              <small><strong>Figure 7. Qualitative comparison with the LLM-based RIS model.</strong></small> 
            </h3>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/long_visual.png" alt="long_quali"/>
            <h3 class="subtitle has-text-centered">
              <small><strong>Figure 8. Qualitative results of our model on long and difficult language inputs.</strong></small> 
            </h3>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End Qualitative -->




<!--BibTex citation -->
  <!--<section class="section" id="BibTeX">-->
    <!--<div class="container is-max-desktop content">-->
      <!--<h2 class="title">BibTeX</h2>-->
      <!--<pre><code>@article{yu2024embedding,
  title={Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation},
  author={Yu, Hyunwoo and Cho, Yubin and Kang, Beoungwoo and Moon, Seunghun and Kong, Kyeongbo and Kang, Suk-Ju},
  journal={arXiv preprint arXiv:2407.17261},
  year={2024}
}</code></pre>-->
    <!--</div>-->
<!--</section>-->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
